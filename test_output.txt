[1m[36m===== TEST 1 =====[0m
[33mPROMPT:[0m yolo what kind of laptop is this

00:02.0 VGA compatible controller: Intel Corporation HD Graphics 620 (rev 02)
model name	: Intel(R) Core(TM) i5-7300U CPU @ 2.60GHz
               total        used        free      shared  buff/cache   available
Mem:            15Gi       4.4Gi       7.5Gi       555Mi       4.5Gi        11Gi
Swap:           12Gi          0B        12Gi
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
sda      8:0    0 238.5G  0 disk 
â”œâ”€sda1   8:1    0   976M  0 part /boot/efi
â”œâ”€sda2   8:2    0 225.2G  0 part /
â””â”€sda3   8:3    0  12.3G  0 part [SWAP]
sdb      8:16   1     0B  0 disk 
20HES0N300
LENOVO
This appears to be a Lenovo ThinkPad laptop, specifically the model 20HES0N300. It has an Intel i5-7300U processor, 16GB of RAM, and a 238.5GB SSD storage. The graphics are handled by Intel HD Graphics 620.
[32mDURATION_S:[0m 33.003

[1m[36m===== END TEST 1 =====[0m

[1m[36m===== TEST 2 =====[0m
[33mPROMPT:[0m yolo how long until midnight?

1756710000
1756706736
54
There are 54 minutes until midnight.
[32mDURATION_S:[0m 13.600

[1m[36m===== END TEST 2 =====[0m

[1m[36m===== TEST 3 =====[0m
[33mPROMPT:[0m yolo what timezone am I in?

PDT
                Time zone: America/Los_Angeles (PDT, -0700)
You are in the timezone: America/Los_Angeles (PDT).
[32mDURATION_S:[0m 12.552

[1m[36m===== END TEST 3 =====[0m

[1m[36m===== TEST 4 =====[0m
[33mPROMPT:[0m yolo what is the CPU temp and battery level?

43000
5
CPU temperature: 43.0Â°C, Battery level: 5%
[32mDURATION_S:[0m 13.439

[1m[36m===== END TEST 4 =====[0m

[1m[36m===== TEST 5 =====[0m
[33mPROMPT:[0m yolo Summarize this repository by listing files in the current directory and describing notable items

total 60
drwxr-xr-x 4 satoshi satoshi 4096 Aug 31 22:49 .
drwxr-xr-x 9 satoshi satoshi 4096 Aug 31 10:42 ..
drwxr-xr-x 8 satoshi satoshi 4096 Aug 31 21:57 .git
-rw-rw-r-- 1 satoshi satoshi   23 Aug 31 18:56 .gitignore
-rwxrwxr-x 1 satoshi satoshi  262 Aug 31 20:51 install
-rw-rw-r-- 1 satoshi satoshi  368 Aug 31 20:52 INSTALLATION.md
-rw-rw-r-- 1 satoshi satoshi 1066 Aug 31 17:58 LICENSE
-rw-rw-r-- 1 satoshi satoshi  465 Aug 31 19:57 pyproject.toml
-rw-r--r-- 1 satoshi satoshi 1366 Aug 31 20:41 README.md
-rw-rw-r-- 1 satoshi satoshi 6142 Aug 31 22:36 RUBBER_DUCKY.md
-rwxrwxr-x 1 satoshi satoshi 2908 Aug 31 23:04 run_tests
drwxrwxr-x 4 satoshi satoshi 4096 Aug 31 21:58 src
-rw-rw-r-- 1 satoshi satoshi 3091 Aug 31 22:35 TESTING.md
-rw-rw-r-- 1 satoshi satoshi 1789 Aug 31 23:06 test_output.txt
src/halp/chat.py
src/halp/__init__.py
src/halp/config.py
src/halp/cli.py
src/halp/VERSION.py
src/halp/api.py
src/halp/ui.py
src/halp/tools.py
src/halp/logging_utils.py
# halp

`halp` is a command line tool that provides AI assistance for your terminal workflows. It connects to an OpenAI-compatible endpoint (e.g., Open WebUI, OpenAI).

## Features

- Simple configuration via `~/.halp.env`
- OpenAI-compatible model listing via `/v1/models`
- CLI overrides for base URL, API key, and model
- Verbose and debug logging
- Secure config file permissions (0600)
- Interactive first-run setup

## Requirements

- Python 3.8+

## Configuration

`halp` stores configuration in `~/.halp.env`. If the file doesnâ€™t exist, the CLI will guide you through a short setup:

```env
BASE_URL=
API_KEY=
DEFAULT_MODEL=
```

# Installation

Ensure pipx is installed

```sh
sudo apt install -y pipx
pipx ensurepath
#restart shell
pipx install git+https://github.com/ABVStudio-net/halp.git

which halp
halp --version
halp -h
```


## Usage

- Print help:
  ```bash
  halp --help
  ```
- Interactive setup:
  ```bash
  halp --init
  ```
- Print current configuration:
  ```bash
  halp --print-config
  ```
- List available models from the configured endpoint:
  ```bash
  halp -l
  ```
- Override model for a single run:
  ```bash
  halp -m hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q8_K_XL
  ```

## Development

- Editable install during development:
  ```bash
  pipx install --editable .
  ```
- Uninstall:
  ```bash
  pipx uninstall halp
  ```
```sh

#TODO: make adjustments for MacOS

sudo apt install -y pipx
pipx ensurepath
#must restart shell, if first time

# uninstall, if installed
if which halp >/dev/null 2>&1; then
  pipx uninstall halp
fi

# FOR DEVELOPMENT
pipx install --editable .

# FOR PRODUCTION
# pipx install git+https://github.com/ABVStudio-net/halp.git

which halp
halp --version
halp -h
```[build-system]
requires = ["setuptools>=61"]
build-backend = "setuptools.build_meta"

[project]
name = "halp"
description = "AI assistance for the command line"
readme = "README.md"
requires-python = ">=3.8"
license = { text = "MIT" }
dynamic = ["version"]

[project.scripts]
halp = "halp.cli:main"

[tool.setuptools]
license-files = ["LICENSE*"]

[tool.setuptools.packages.find]
where = ["src"]

[tool.setuptools.dynamic]
version = {attr = "halp.VERSION.VERSION"}
# RUBBER_DUCKY: halp Developer Onboarding

This document orients a new contributor to the halp CLI: how it works, how itâ€™s organized, and where to extend or clean up. Read top-to-bottom in one sitting.

## What halp is
- __Purpose__: AI assistant for the command line that talks to an OpenAI-compatible API and supports an Agent mode to run tools (e.g., shell commands) with safety controls.
- __Install/Run__: packaged via `pyproject.toml` with console script `halp = halp.cli:main`.

## High-level execution flow
1. __Entry__: `halp.cli:main()` parses CLI flags.
2. __Config__: Loads `~/.halp.env` via `halp.config.ensure_config()` and applies CLI overrides.
3. __Mode__:
   - Chat mode (default): `halp.chat.chat_loop()` streams model output to the terminal.
   - Agent mode (`--agent`): `halp.chat.agent_loop()` runs a ReACT-style loop with JSON tool-calls.
4. __HTTP__: Model interaction over OpenAI-compatible endpoints in `halp.api`.
5. __Tools__: Agent executes actions via registry from `halp.tools` (currently `shell`).

## Repository layout
- `pyproject.toml` â€” package metadata, entry point
- `src/halp/`
  - `cli.py` â€” CLI entrypoint and argument parsing
  - `chat.py` â€” chat loop and agent loop
  - `api.py` â€” HTTP helpers for `/v1/models` and `/v1/chat/completions` (stream)
  - `config.py` â€” `.halp.env` read/write, interactive setup
  - `logging_utils.py` â€” colored logging and helpers
  - `tools.py` â€” `Tool` base, `ShellTool`, `get_default_toolset()`
  - `ui.py` â€” TTY-safe `read_line_interactive()` for prompts
  - `VERSION.py`, `__init__.py` â€” version export

## Key modules and responsibilities
- `halp.cli.main()`
  - Flags: `--env`, `--verbose`, `--debug`, `--init`, `--yolo`, overrides (`--base_url`, `--api_key`, `--model`), actions (`-l/--list_models`), __agent flags__ (`--agent`, `--max_steps`, `--unsafe_exec`).
  - Flow: ensure config â†’ optional list models â†’ compute prompt (args/stdin) â†’ choose chat or agent loop.
- `halp.chat.chat_loop()`
  - Maintains `messages` and streams assistant tokens (SSE) with green color.
  - Returns 0 on success, 130 on Ctrl-C, 1 on request failure.
- `halp.chat.agent_loop()`
  - ReACT skeleton with step limit (`max_steps`).
  - Expects the model to emit JSON tool calls or a final object, e.g.:
    - Tool: `{ "tool": "shell", "input": "ls -la" }`
    - Final: `{ "final": "â€¦your answerâ€¦" }`
  - Parses JSON from ```json code fences or first `{...}` block.
  - Executes the tool from a registry and appends an `Observation` back to the transcript:
    - Assistant: original reply (for traceability)
    - User: `Observation:\n{"tool": "shell", "result": { ... }}`
- `halp.api`
  - `list_models_openai()` â€” GET models list.
  - `chat_completion_openai_stream()` â€” POST to chat completions and yield SSE chunks.
- `halp.tools`
  - `Tool` base class: `run(user_input: str) -> dict` returning `{ok, returncode, stdout, stderr}`.
  - `ShellTool`
    - Safety: regex blocklist (e.g., `rm`, `sudo`, package managers, redirections, etc.).
    - Interactive confirmation before executing unless `--unsafe_exec` is set.
  - `get_default_toolset(unsafe_exec)` returns `{ "shell": ShellTool(...) }`.
- `halp.config`
  - `.halp.env` keys: `BASE_URL`, `API_KEY`, `DEFAULT_MODEL`.
  - Interactive setup supports defaults from `HALP_BASE_URL` and `HALP_DEFAULT_MODEL` env vars.
- `halp.logging_utils`
  - `setup_logging(debug)` sets up colored console logs (DEBUG â†’ red).
- `halp.ui`
  - `read_line_interactive()` uses TTY when stdin is not a TTY, with colorized prompts and safe resets.

## CLI Quickstart
- Help: `halp -h`
- First-time config: `halp --init`
- Print config: `halp --env`  (Note: README currently shows `--print-config`; see Cleanup)
- List models: `halp -l`
- One-off model override: `halp -m <model>`
- Chat mode interactive: `halp`
- Chat mode one reply: `halp -o "Explain pipes in bash"`
- Agent mode (dangerous; opt-in): `halp --agent --unsafe_exec "Remove temp files"`

## Agent JSON protocols (what the model should emit)
- __Tool call__
  ```json
  { "tool": "shell", "input": "ls -la" }
  ```
- __Final answer__
  ```json
  { "final": "Your concise answer." }
  ```
- The loop returns when a final object is seen, or when `max_steps` is reached.

## Safety model
- Default: cautious. The shell requires interactive confirmation before running.
- `--unsafe_exec`: explicitly allow auto-execution â€” users must opt in (or use `yolo` directive).

## Extending halp
- __Add a new tool__
  - Create a new subclass of `Tool` in `tools.py` with a unique `name`.
  - Implement `run(user_input: str) -> dict`.
  - Register it in `get_default_toolset()` (or introduce a plugin registry).
  - Update the agent prompt to document the new tool name and its expected input format.

## Testing locally
- Ensure env: `~/.halp.env` with `BASE_URL`, `API_KEY`, `DEFAULT_MODEL`.
- Connectivity: `halp -l` should print models.
- Chat smoke test: `echo "hello" | halp -o`

## Known gaps / cleanup opportunities
- __README flag drift__: README uses `--print-config`; CLI flag is `--env`. Align docs or add an alias.
- __SSE dependency__: `chat_completion_openai_stream()` assumes streaming support. Some providers differ; consider fallback non-streaming path on error.
- __Agent brittleness__: relies on strict JSON emission. Consider structured outputs or tool schema few-shot examples in the system prompt.
- __ShellTool blocklist__: expand/maintain; consider an allowlist mode, path sandboxing, or `cwd` constraints.
- __Error surfaces__: chat loop prints generic failure; propagate brief reason from HTTP layer when safe.
- __Observations format__: currently a simple JSON blob in text; consider a dedicated role or metadata channel when supported.

## Glossary of important symbols
- `chat_loop(base_url, api_key, model, initial_user_prompt, system_prompt, once, logger)`
- `agent_loop(base_url, api_key, model, initial_user_prompt, system_prompt, tools, max_steps, logger)`
- `get_default_toolset(unsafe_exec)` â†’ `{ name: Tool }`
- `chat_completion_openai_stream(base_url, api_key, model, messages, ...)` â†’ yields text chunks

â€” End â€”
# TESTING halp

This guide provides 15 example commands to exercise halpâ€™s default agent mode, quick mode, confirmation gating, and the "yolo" auto-execution directive.

Important:
- By default, any CLI command the agent wants to run will request your confirmation.
- "yolo" as the first word of your prompt enables auto-execution (equivalent to --unsafe_exec for the session) and is stripped from the task.
- `--unsafe_exec` enables auto-execution for all commands without confirmation.
- `--quick` produces a single one-pass reply without using tools.

Before testing:
- Ensure `BASE_URL`, `API_KEY`, and `DEFAULT_MODEL` are configured (`~/.halp.env` or CLI overrides).
- Review `halp --help` for all options.

## 15 Example Tests

1) Default agent: directory overview (should ask to approve a safe command like `ls`)
   
   ```bash
   halp "List the current directory with details and summarize notable items"
   ```

2) Decline confirmation path (answer "n" when prompted)
   
   ```bash
   halp "Check disk usage of the current directory and suggest cleanup options"
   ```

3) Unsafe exec flag (auto-exec, no confirmations)
   
   ```bash
   halp --unsafe_exec "Show the top 10 largest files under the current directory recursively"
   ```

4) Quick mode (no tools, single-pass)
   
   ```bash
   halp --quick "Explain what this repository does, based on filenames only"
   ```

5) YOLO auto-exec (no confirmations)
   
   ```bash
   halp "yolo show disk usage summary of the current directory"
   ```

6) YOLO via piped input (auto-exec)
   
   ```bash
   echo "yolo print working directory and current user" | halp
   ```

7) Summarize recent shell history (tests system prompt history injection)
   
   ```bash
   halp "Summarize my recent shell activity and infer what Iâ€™ve been working on"
   ```

8) Environment grounding (OS, Python, Shell, CWD)
   
   ```bash
   halp "What OS, Python version, and shell am I using? Use the environment info provided"
   ```

9) Read a file and summarize (should propose a safe file read command)
   
   ```bash
   halp "Read the first 15 lines of README.md and summarize the project goals"
   ```

10) Git status summary (non-destructive)
    
    ```bash
    halp "Summarize the current Git status and list untracked files"
    ```

11) Processes by memory (auto-exec with yolo)
    
    ```bash
    halp "yolo list the top 5 processes by memory usage"
    ```

12) Check port usage (safe)
    
    ```bash
    halp "Check if port 8000 is listening and show matching processes"
    ```

13) Find Python files count under src/
    
    ```bash
    halp "Count how many Python files are under src/ and print the total"
    ```

14) List available models (API integration)
    
    ```bash
    halp -l
    ```

15) Interactive start + yolo directive
    
    ```bash
    halp
    # When prompted: yolo show the top 10 largest files under the repo
    # Confirmations should be skipped for this session.
    ```

Notes:
- If you want to avoid approving commands interactively in non-tty contexts, use `--unsafe_exec` or the `yolo` directive.
"""
halp package

Provides CLI entrypoint in halp.cli and exposes __version__.
"""
from .VERSION import VERSION as __version__

__all__ = ["__version__"]
# Package version for halp
VERSION = "0.1.1"
#!/usr/bin/env python3
"""
halp - AI assistance for the command line.

This CLI uses a simple .env file at ~/.halp.env for configuration with keys:
- BASE_URL: OpenAI-compatible base URL (e.g., https://api.openai.com)
- API_KEY: API key for the provider
- DEFAULT_MODEL: Default model to use

You can override these at runtime via flags, and list available models from an
OpenAI-compatible endpoint using --list_models.
"""

import argparse
import sys
import json
import os
import platform
import getpass

from . import __version__
from .logging_utils import setup_logging as setup_logging_new
from .config import ensure_config as ensure_config_new
from .api import list_models_openai as list_models_openai_new
from .chat import chat_loop as chat_loop_new
from .chat import agent_loop as agent_loop_new
from .tools import get_default_toolset

def main(argv=None) -> int:  # new modular entrypoint (overrides legacy main above)
    parser = argparse.ArgumentParser(
Agent reached max steps without finishing.
[32mDURATION_S:[0m 47.766
[1m[31mEXIT_STATUS:[0m 1

[1m[36m===== END TEST 5 =====[0m

[1m[36m===== TEST 6 =====[0m
[33mPROMPT:[0m yolo Show disk usage of the current directory, sorted descending, and print the top 10 entries

168K	src
16K	test_output.txt
8.0K	RUBBER_DUCKY.md
4.0K	TESTING.md
4.0K	run_tests
4.0K	README.md
4.0K	pyproject.toml
4.0K	LICENSE
4.0K	INSTALLATION.md
4.0K	install
Here are the top 10 largest entries in the current directory, sorted by size in descending order:

1. src - 168K
2. test_output.txt - 16K
3. RUBBER_DUCKY.md - 8.0K
4. TESTING.md - 4.0K
5. run_tests - 4.0K
6. README.md - 4.0K
7. pyproject.toml - 4.0K
8. LICENSE - 4.0K
9. INSTALLATION.md - 4.0K
10. install - 4.0K
[32mDURATION_S:[0m 9.532

[1m[36m===== END TEST 6 =====[0m

[1m[36m===== TEST 7 =====[0m
[33mPROMPT:[0m yolo Find the 10 largest files under the repository recursively with human-readable sizes

Agent reached max steps without finishing.
[32mDURATION_S:[0m 46.031
[1m[31mEXIT_STATUS:[0m 1

[1m[36m===== END TEST 7 =====[0m

[1m[36m===== TEST 8 =====[0m
[33mPROMPT:[0m yolo Count total lines of code for Python files under src/

  295 src/halp/chat.py
    8 src/halp/__init__.py
  126 src/halp/config.py
  279 src/halp/cli.py
    2 src/halp/VERSION.py
  177 src/halp/api.py
   57 src/halp/ui.py
  160 src/halp/tools.py
   79 src/halp/logging_utils.py
 1183 total
Total lines of code for Python files under src/: 1183 lines
Breakdown:
- src/halp/chat.py: 295 lines
- src/halp/__init__.py: 8 lines
- src/halp/config.py: 126 lines
- src/halp/cli.py: 279 lines
- src/halp/VERSION.py: 2 lines
- src/halp/api.py: 177 lines
- src/halp/ui.py: 57 lines
- src/halp/tools.py: 160 lines
- src/halp/logging_utils.py: 79 lines
[32mDURATION_S:[0m 10.366

[1m[36m===== END TEST 8 =====[0m

[1m[36m===== TEST 9 =====[0m
[33mPROMPT:[0m yolo Show the 10 most CPU-intensive processes (no sudo)

